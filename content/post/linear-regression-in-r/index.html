---
title: Linear regression in r
author: ''
date: '2020-08-21'
slug: linear-regression-in-r
categories:
  - r programming
tags: []
subtitle: ''
summary: 'Building a linear regression model in r using both the base r and the tidymodels approach'
authors: []
lastmod: '2020-08-21T11:46:19+01:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: true
projects: []
---



<p>In this lesson we will try and get a high level intuitive understanding of how linear regression works.
Regression analysis is often used to model causes and effects, the cause being the independent variable and can also be referred to as the explanatory/predictor variable (<code>X</code>), the effect is the dependent variable i.e the variable that is dependent on the cause.</p>
<div id="lets-consider-simple-linear-regression" class="section level2">
<h2>Let’s consider simple linear regression;</h2>
<p>Simple linear regression is a regression where we have exactly 1 <code>X</code> variable. Simple linear regression is easy to understand because the data points used for the regression analysis can be plotted on a two dimensional plane like in the diagram below</p>
<p><img src="linear-regression-in-r/simple_linear_regression.png" /></p>
</div>
<div id="so-what-exactly-is-linear-regression" class="section level2">
<h2>So what exactly is linear regression?</h2>
<p>Linear regression involves finding the best fit line through the data points</p>
<p><img src="linear-regression-in-r/simple_linear_regression_bestfit.png" /></p>
<p>The line is what is used for prediction.</p>
</div>
<div id="so-what-is-the-regression-line" class="section level2">
<h2>So what is the regression line?</h2>
<p>The regression line also known as best fit line is that line which minimizes the variance of the residuals (Mean Square Error). The residuals in this case are the difference between the actual values of the dependent variable and the fitted values of the dependent variable.</p>
<p>When there are more than one explanatory variable the regression becomes <em>Multiple Regression</em>. The same objective of the simple linear regression applies to the multiple regression, i.e To minimize the variance of the residuals.</p>
<p>Now after performing regression analysis (whether simple linear regression or multiple regression) we need a way to be able to evaluate our result, one of the ways we can evaluate the result is by using the <span class="math inline">\(R^{2}\)</span> metric<br />
<span class="math inline">\(R^{2}\)</span> = ESS / TSS<br />
where<br />
<code>ESS</code> stands for EXPLAINED SUM OF SQUARES (variance of the fitted values)<br />
<code>TSS</code> stands for TOTAL SUM OF SQUARES (variance of the actual values)<br />
The <span class="math inline">\(R^{2}\)</span> metric measures the percentage of total variance explained by the regression. The higher the <span class="math inline">\(R^{2}\)</span> the better the quality of the regression analysis.</p>
<p>In multiple regression the <span class="math inline">\(R^{2}\)</span> metric is not used directly, we make use of the adjusted <span class="math inline">\(R^{2}\)</span>, and this is because in multiple regression there may be variables which are not relevant. The adjusted <span class="math inline">\(R^{2}\)</span> adds an additional penalty to handle variables that are not really relevant in the analysis.</p>
<p>In summary, the best fit regression line is the line that produces the best <span class="math inline">\(R^{2}\)</span></p>
<p>Another way to evaluate the result of a regression analysis is by using the <code>RMSE</code> metric. The root mean square error is a preferred evaluation choice for some people, it can be obtained by taking the root of the Mean Square Error.</p>
</div>
<div id="example" class="section level1">
<h1>EXAMPLE</h1>
<p>Now that we know what regression analysis is and how to evaluate the result let us use an example to gain a practical knowledge of what we have discussed.</p>
<p>For this example we are going to build the linear regression model first using base R and then using tidymodels..</p>
<p>We will be making use of the <code>Real Estate Dataset</code> from kaggle for this example. Our aim is to predict house price given some features like <em>date of purchase</em>, <em>house age</em>, <em>number of convinience stores</em> ,<em>location</em>, and <em>distance to nearest mrt station</em>.</p>
<p>Before proceeding let us make some few assumptions about the dataset since we didn’t get much information about the dataset from kaggle.
Bellow are the assumptions;<br />
1. The house price of unit area is in thousands of dollars<br />
2. The distance to nearest mrt station is distance in miles</p>
<p>Now let us load all the packages that we will need for this example</p>
<pre class="r"><code>require(tidyverse)
require(tidymodels)
require(GGally)</code></pre>
<div id="read-data-into-rstudio-environment" class="section level3">
<h3>Read data into rstudio environment</h3>
<pre class="r"><code>real_estate &lt;- read_csv(&quot;https://raw.githubusercontent.com/twirelex/dataset/master/real_estate.csv&quot;) %&gt;% janitor::clean_names()</code></pre>
<pre><code>## Parsed with column specification:
## cols(
##   No = col_double(),
##   `X1 transaction date` = col_double(),
##   `X2 house age` = col_double(),
##   `X3 distance to the nearest MRT station` = col_double(),
##   `X4 number of convenience stores` = col_double(),
##   `X5 latitude` = col_double(),
##   `X6 longitude` = col_double(),
##   `Y house price of unit area` = col_double()
## )</code></pre>
</div>
<div id="view-data-types-and-see-summary-statistics" class="section level3">
<h3>View data types and see summary statistics</h3>
<pre class="r"><code>dim(real_estate)</code></pre>
<pre><code>## [1] 414   8</code></pre>
<pre class="r"><code>glimpse(real_estate)</code></pre>
<pre><code>## Rows: 414
## Columns: 8
## $ no                                     &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 1...
## $ x1_transaction_date                    &lt;dbl&gt; 2012.917, 2012.917, 2013.583...
## $ x2_house_age                           &lt;dbl&gt; 32.0, 19.5, 13.3, 13.3, 5.0,...
## $ x3_distance_to_the_nearest_mrt_station &lt;dbl&gt; 84.87882, 306.59470, 561.984...
## $ x4_number_of_convenience_stores        &lt;dbl&gt; 10, 9, 5, 5, 5, 3, 7, 6, 1, ...
## $ x5_latitude                            &lt;dbl&gt; 24.98298, 24.98034, 24.98746...
## $ x6_longitude                           &lt;dbl&gt; 121.5402, 121.5395, 121.5439...
## $ y_house_price_of_unit_area             &lt;dbl&gt; 37.9, 42.2, 47.3, 54.8, 43.1...</code></pre>
<p><strong>remove the <code>no</code> column as it is only an identifier and doesn’t really need to be part of the variables</strong></p>
<pre class="r"><code>real_estate &lt;- real_estate %&gt;% 
  select(-no)</code></pre>
<pre class="r"><code>knitr::kable(head(real_estate))</code></pre>
<table>
<thead>
<tr class="header">
<th align="right">x1_transaction_date</th>
<th align="right">x2_house_age</th>
<th align="right">x3_distance_to_the_nearest_mrt_station</th>
<th align="right">x4_number_of_convenience_stores</th>
<th align="right">x5_latitude</th>
<th align="right">x6_longitude</th>
<th align="right">y_house_price_of_unit_area</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">2012.917</td>
<td align="right">32.0</td>
<td align="right">84.87882</td>
<td align="right">10</td>
<td align="right">24.98298</td>
<td align="right">121.5402</td>
<td align="right">37.9</td>
</tr>
<tr class="even">
<td align="right">2012.917</td>
<td align="right">19.5</td>
<td align="right">306.59470</td>
<td align="right">9</td>
<td align="right">24.98034</td>
<td align="right">121.5395</td>
<td align="right">42.2</td>
</tr>
<tr class="odd">
<td align="right">2013.583</td>
<td align="right">13.3</td>
<td align="right">561.98450</td>
<td align="right">5</td>
<td align="right">24.98746</td>
<td align="right">121.5439</td>
<td align="right">47.3</td>
</tr>
<tr class="even">
<td align="right">2013.500</td>
<td align="right">13.3</td>
<td align="right">561.98450</td>
<td align="right">5</td>
<td align="right">24.98746</td>
<td align="right">121.5439</td>
<td align="right">54.8</td>
</tr>
<tr class="odd">
<td align="right">2012.833</td>
<td align="right">5.0</td>
<td align="right">390.56840</td>
<td align="right">5</td>
<td align="right">24.97937</td>
<td align="right">121.5425</td>
<td align="right">43.1</td>
</tr>
<tr class="even">
<td align="right">2012.667</td>
<td align="right">7.1</td>
<td align="right">2175.03000</td>
<td align="right">3</td>
<td align="right">24.96305</td>
<td align="right">121.5125</td>
<td align="right">32.1</td>
</tr>
</tbody>
</table>
</div>
<div id="exploratory-data-analysis" class="section level3">
<h3>EXPLORATORY DATA ANALYSIS</h3>
<pre class="r"><code>summary(real_estate)</code></pre>
<pre><code>##  x1_transaction_date  x2_house_age    x3_distance_to_the_nearest_mrt_station
##  Min.   :2013        Min.   : 0.000   Min.   :  23.38                       
##  1st Qu.:2013        1st Qu.: 9.025   1st Qu.: 289.32                       
##  Median :2013        Median :16.100   Median : 492.23                       
##  Mean   :2013        Mean   :17.713   Mean   :1083.89                       
##  3rd Qu.:2013        3rd Qu.:28.150   3rd Qu.:1454.28                       
##  Max.   :2014        Max.   :43.800   Max.   :6488.02                       
##  x4_number_of_convenience_stores  x5_latitude     x6_longitude  
##  Min.   : 0.000                  Min.   :24.93   Min.   :121.5  
##  1st Qu.: 1.000                  1st Qu.:24.96   1st Qu.:121.5  
##  Median : 4.000                  Median :24.97   Median :121.5  
##  Mean   : 4.094                  Mean   :24.97   Mean   :121.5  
##  3rd Qu.: 6.000                  3rd Qu.:24.98   3rd Qu.:121.5  
##  Max.   :10.000                  Max.   :25.01   Max.   :121.6  
##  y_house_price_of_unit_area
##  Min.   :  7.60            
##  1st Qu.: 27.70            
##  Median : 38.45            
##  Mean   : 37.98            
##  3rd Qu.: 46.60            
##  Max.   :117.50</code></pre>
<p>Looking at the summary statistics above we can see that variables like <code>distance_to_the_nearest_mrt_station</code> has a mean value that is very high when compared to other variables like the <code>number_of_convinience_stores</code> and <code>house_age</code>, these large values will have effect on our model if used as is, so we will have to standardize the variables in the pre-processing stage before building our model so that all the variables can have values that will not cause bias in our model.</p>
<pre class="r"><code>ggpairs(real_estate, upper = NULL)</code></pre>
<p><img src="/post/linear-regression-in-r/index_files/figure-html/unnamed-chunk-8-1.png" width="1728" /></p>
<p>These are some of the things that can be observed from the plot above<br />
* Most of the variables are not normally distributed
* The <code>distance_to_the_nearest_mrt_station</code> has a negative correlation with the <code>house price</code>
* The <code>transaction date</code> variable doesn’t really have effect on the <code>house price</code>
* The <code>house age</code> variable has a negative correlation with the <code>house price</code><br />
* The <code>number_of_convinience_stores</code> has a fairly positive correlation with the <code>house price</code></p>
</div>
<div id="data-pre-processing" class="section level3">
<h3>DATA PRE-PROCESSING</h3>
<p>The only pre-processing we will do before building our model is standardization. We want our variables to have a mean close to zero and a standard deviation of 1. We will do this with the scale function in r.</p>
<pre class="r"><code>real_estate_standard &lt;- cbind(price = real_estate$y_house_price_of_unit_area, data.frame(scale(real_estate[1:6])))</code></pre>
<p><strong>verify that the explanatory variables have been standardized</strong></p>
<pre class="r"><code>knitr::kable(head(real_estate_standard))</code></pre>
<table>
<thead>
<tr class="header">
<th align="right">price</th>
<th align="right">x1_transaction_date</th>
<th align="right">x2_house_age</th>
<th align="right">x3_distance_to_the_nearest_mrt_station</th>
<th align="right">x4_number_of_convenience_stores</th>
<th align="right">x5_latitude</th>
<th align="right">x6_longitude</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">37.9</td>
<td align="right">-0.8226878</td>
<td align="right">1.2541110</td>
<td align="right">-0.7915373</td>
<td align="right">2.0049816</td>
<td align="right">1.1240698</td>
<td align="right">0.4482199</td>
</tr>
<tr class="even">
<td align="right">42.2</td>
<td align="right">-0.8226878</td>
<td align="right">0.1568964</td>
<td align="right">-0.6158665</td>
<td align="right">1.6654877</td>
<td align="right">0.9113415</td>
<td align="right">0.4006542</td>
</tr>
<tr class="odd">
<td align="right">47.3</td>
<td align="right">1.5392887</td>
<td align="right">-0.3873220</td>
<td align="right">-0.4135150</td>
<td align="right">0.3075125</td>
<td align="right">1.4850633</td>
<td align="right">0.6873517</td>
</tr>
<tr class="even">
<td align="right">54.8</td>
<td align="right">1.2449283</td>
<td align="right">-0.3873220</td>
<td align="right">-0.4135150</td>
<td align="right">0.3075125</td>
<td align="right">1.4850633</td>
<td align="right">0.6873517</td>
</tr>
<tr class="odd">
<td align="right">43.1</td>
<td align="right">-1.1205948</td>
<td align="right">-1.1158725</td>
<td align="right">-0.5493321</td>
<td align="right">0.3075125</td>
<td align="right">0.8331800</td>
<td align="right">0.5922203</td>
</tr>
<tr class="even">
<td align="right">32.1</td>
<td align="right">-1.7093156</td>
<td align="right">-0.9315405</td>
<td align="right">0.8645401</td>
<td align="right">-0.3714751</td>
<td align="right">-0.4818677</td>
<td align="right">-1.3566716</td>
</tr>
</tbody>
</table>
</div>
<div id="modeling-the-base-r-way" class="section level3">
<h3>MODELING THE BASE R WAY</h3>
<p>The <code>lm</code> function can be used to perform regression analysis in r.</p>
<p>Let us first build a simple linear regression where there is only one explanatory/independent variable.</p>
<pre class="r"><code>set.seed(11)

simp_model &lt;- lm(price~ x2_house_age, data = real_estate_standard)</code></pre>
<p>We can use the summary function in r to view information about the model</p>
<pre class="r"><code>summary(simp_model)</code></pre>
<pre><code>## 
## Call:
## lm(formula = price ~ x2_house_age, data = real_estate_standard)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -31.113 -10.738   1.626   8.199  77.781 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)   37.9802     0.6545  58.027  &lt; 2e-16 ***
## x2_house_age  -2.8651     0.6553  -4.372 1.56e-05 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 13.32 on 412 degrees of freedom
## Multiple R-squared:  0.04434,    Adjusted R-squared:  0.04202 
## F-statistic: 19.11 on 1 and 412 DF,  p-value: 1.56e-05</code></pre>
<p>For this model we got an R-squared of 0.04202, house age has a negative correlation with the house price. The p-value also tells us that the house age variable is a significant variable.</p>
<p><strong>Now let us build a multiple linear regression model where there are more than one independent variable.</strong></p>
<pre class="r"><code>set.seed(111)

mul_model &lt;- lm(price~., data = real_estate_standard)</code></pre>
<pre class="r"><code>summary(mul_model)</code></pre>
<pre><code>## 
## Call:
## lm(formula = price ~ ., data = real_estate_standard)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -35.664  -5.410  -0.966   4.217  75.193 
## 
## Coefficients:
##                                        Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept)                             37.9802     0.4353  87.244  &lt; 2e-16 ***
## x1_transaction_date                      1.4511     0.4390   3.305  0.00103 ** 
## x2_house_age                            -3.0725     0.4390  -7.000 1.06e-11 ***
## x3_distance_to_the_nearest_mrt_station  -5.6637     0.9062  -6.250 1.04e-09 ***
## x4_number_of_convenience_stores          3.3381     0.5542   6.023 3.84e-09 ***
## x5_latitude                              2.7982     0.5531   5.059 6.38e-07 ***
## x6_longitude                            -0.1907     0.7456  -0.256  0.79829    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 8.858 on 407 degrees of freedom
## Multiple R-squared:  0.5824, Adjusted R-squared:  0.5762 
## F-statistic: 94.59 on 6 and 407 DF,  p-value: &lt; 2.2e-16</code></pre>
<p>We can see that there is an improvement in the adjusted R-squared after we used all the independent variables. The <code>longitude</code> and <code>transaction date</code> variables didn’t really have much significant effect in the model considering their p-values.</p>
<p><strong>Now let us calculate the root mean square error</strong></p>
<pre class="r"><code>sqrt(mean(mul_model$residuals^2))</code></pre>
<pre><code>## [1] 8.782466</code></pre>
</div>
<div id="modeling-the-tidymodels-way" class="section level3">
<h3>MODELING THE TIDYMODELS WAY</h3>
<p>Let us now build the same type of multiple regression that we built in base r using tidymodels</p>
<p><strong>We will first pre-process the data using recipe</strong></p>
<pre class="r"><code>my_rec &lt;- recipe(y_house_price_of_unit_area~., data = real_estate) %&gt;% 
  step_normalize(all_predictors()) %&gt;% 
  prep()

real_estate_standard2 &lt;- bake(my_rec, real_estate)</code></pre>
<p>the above process is similar to the one we did in base r with the scale function.</p>
<p><strong>view and verify the pre-processed data</strong></p>
<pre class="r"><code>knitr::kable(head(real_estate_standard2))</code></pre>
<table>
<thead>
<tr class="header">
<th align="right">x1_transaction_date</th>
<th align="right">x2_house_age</th>
<th align="right">x3_distance_to_the_nearest_mrt_station</th>
<th align="right">x4_number_of_convenience_stores</th>
<th align="right">x5_latitude</th>
<th align="right">x6_longitude</th>
<th align="right">y_house_price_of_unit_area</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">-0.8226878</td>
<td align="right">1.2541110</td>
<td align="right">-0.7915373</td>
<td align="right">2.0049816</td>
<td align="right">1.1240698</td>
<td align="right">0.4482199</td>
<td align="right">37.9</td>
</tr>
<tr class="even">
<td align="right">-0.8226878</td>
<td align="right">0.1568964</td>
<td align="right">-0.6158665</td>
<td align="right">1.6654877</td>
<td align="right">0.9113415</td>
<td align="right">0.4006542</td>
<td align="right">42.2</td>
</tr>
<tr class="odd">
<td align="right">1.5392887</td>
<td align="right">-0.3873220</td>
<td align="right">-0.4135150</td>
<td align="right">0.3075125</td>
<td align="right">1.4850633</td>
<td align="right">0.6873517</td>
<td align="right">47.3</td>
</tr>
<tr class="even">
<td align="right">1.2449283</td>
<td align="right">-0.3873220</td>
<td align="right">-0.4135150</td>
<td align="right">0.3075125</td>
<td align="right">1.4850633</td>
<td align="right">0.6873517</td>
<td align="right">54.8</td>
</tr>
<tr class="odd">
<td align="right">-1.1205948</td>
<td align="right">-1.1158725</td>
<td align="right">-0.5493321</td>
<td align="right">0.3075125</td>
<td align="right">0.8331800</td>
<td align="right">0.5922203</td>
<td align="right">43.1</td>
</tr>
<tr class="even">
<td align="right">-1.7093156</td>
<td align="right">-0.9315405</td>
<td align="right">0.8645401</td>
<td align="right">-0.3714751</td>
<td align="right">-0.4818677</td>
<td align="right">-1.3566716</td>
<td align="right">32.1</td>
</tr>
</tbody>
</table>
<div id="build-model" class="section level4">
<h4>build model</h4>
<p>We will build the model using the <code>glmnet</code> engine so that wen tune some hyper-parameters</p>
<pre class="r"><code>model &lt;- linear_reg(penalty = tune(), mixture = tune() ) %&gt;% 
  set_engine(&quot;glmnet&quot;)</code></pre>
<p>Now we will use the grid_max_entropy function to make a combination of values that we would like to try for both the <code>penalty</code> and the <code>mixture</code> hyper-parameters</p>
<pre class="r"><code>model_grid &lt;- grid_max_entropy(penalty(), mixture(), size = 20)

knitr::kable(head(model_grid, 10))</code></pre>
<table>
<thead>
<tr class="header">
<th align="right">penalty</th>
<th align="right">mixture</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="right">0.0000000</td>
<td align="right">0.4276270</td>
</tr>
<tr class="even">
<td align="right">0.7664484</td>
<td align="right">0.2547693</td>
</tr>
<tr class="odd">
<td align="right">0.0015308</td>
<td align="right">0.3820641</td>
</tr>
<tr class="even">
<td align="right">0.0000023</td>
<td align="right">0.2363285</td>
</tr>
<tr class="odd">
<td align="right">0.0000000</td>
<td align="right">0.7625557</td>
</tr>
<tr class="even">
<td align="right">0.0000002</td>
<td align="right">0.0154776</td>
</tr>
<tr class="odd">
<td align="right">0.0000000</td>
<td align="right">0.1079748</td>
</tr>
<tr class="even">
<td align="right">0.0002425</td>
<td align="right">0.6673050</td>
</tr>
<tr class="odd">
<td align="right">0.0007739</td>
<td align="right">0.1313791</td>
</tr>
<tr class="even">
<td align="right">0.0000001</td>
<td align="right">0.7554802</td>
</tr>
</tbody>
</table>
<p><strong>Create a 10-folds cross-validation</strong></p>
<pre class="r"><code>model_cv &lt;- vfold_cv(real_estate_standard2, strata = y_house_price_of_unit_area, v = 10)</code></pre>
<p><strong>So that we don’t wait too long we will activated parallel processing</strong></p>
<pre class="r"><code>require(doSNOW)

cl &lt;- makeCluster(4, type = &quot;SOCK&quot;)

registerDoSNOW(cl)</code></pre>
<p><strong>Now we tune our specifications using the tune_grid function</strong></p>
<pre class="r"><code>tuning &lt;- tune_grid(model,y_house_price_of_unit_area~.,resamples = model_cv, grid = model_grid)</code></pre>
<p><strong>We will use the select_best function to select the best values for our hyper-parameters so that we can use those values to build our model</strong></p>
<pre class="r"><code>best_tune &lt;- tuning %&gt;% select_best()</code></pre>
<pre><code>## Warning: No value of `metric` was given; metric &#39;rmse&#39; will be used.</code></pre>
<p><strong>We can now finalize our model by inserting those values we obtained from the select_best function into the <code>penalty</code> and <code>mixture</code> parameters that we didn’t specify any value for initially</strong></p>
<pre class="r"><code>model &lt;- model %&gt;% finalize_model(best_tune)</code></pre>
<p><strong>Now we fit the model and make predictions</strong></p>
<pre class="r"><code>model &lt;- model %&gt;% fit(y_house_price_of_unit_area~., data = real_estate_standard2) %&gt;% predict(real_estate_standard2) %&gt;% mutate(truth = real_estate_standard2$y_house_price_of_unit_area)</code></pre>
<p><strong>Model Evaluation</strong></p>
<pre class="r"><code>model %&gt;% metrics(truth = truth, estimate = .pred)</code></pre>
<pre><code>## # A tibble: 3 x 3
##   .metric .estimator .estimate
##   &lt;chr&gt;   &lt;chr&gt;          &lt;dbl&gt;
## 1 rmse    standard       8.79 
## 2 rsq     standard       0.582
## 3 mae     standard       6.14</code></pre>
<p>Though there is no improvement in the model after using tidymodels and tuning for penalty and mixture, there are ways we can try to improve the model but that wasn’t what our goal was for this lesson.</p>
</div>
</div>
<div id="conclusion" class="section level3">
<h3>CONCLUSION</h3>
<p>R provides us with a convenient way of performing regression analysis and with the help of tidymodels we can do more advanced and robust analysis like applying bootstrap resampling, cross-validation etc..</p>
</div>
</div>
