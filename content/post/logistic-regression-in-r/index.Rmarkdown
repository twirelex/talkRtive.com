---
title: Logistic regression in r
author: ''
date: '2023-06-02'
slug: logistic-regression-in-r
categories:
  - r programming
tags: []
subtitle: ''
summary: 'In this lesson we will try to understand Logistic Regression in r and how to build a predictive model with it.'
authors: []
lastmod: '2023-06-02T17:06:56+01:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: true
projects: []
---  

```{r, include=FALSE}
knitr::knit_hooks$set(
  plot = function(x, options) {
    hugoopts <- options$hugoopts
    paste0(
      "{", "{<figure src=", # the original code is simpler
      # but here I need to escape the shortcode!
      '"', x, '" ',
      if (!is.null(hugoopts)) {
        glue::glue_collapse(
          glue::glue('{names(hugoopts)}="{hugoopts}"'),
          sep = " "
        )
      },
      ">}}\n"
    )
  }
)

```


In this lesson we will try and get an intuitive understanding of how Logistic Regression works.  
Logistic regression can be used for both regression and classification machine learning problems i.e When the dependent/response variable is continuous or discrete, but it does better when the dependent variable is discrete(Binary) i.e yes or no, good or bad, 0 or 1 etc....  

## So what exactly is Logistic regression?
I found a good definition on <a href="https://simple.wikipedia.org/wiki/Logistic_Regression" target="_blank" rel="nofollow noopener"> wikipedia </a>  
"Logistic Regression, also known as Logit Regression or Logit Model, is a mathematical model used in statistics to estimate (guess) the probability of an event occurring having been given some previous data".  

Unlike [linear regression]({{< ref "post/linear-regression-in-r/index.Rmarkdown" >}}) that tries to make predictions by finding a linear (straight line) equation to model or predict future data points. Logistic regression does not look at the relationship between the two variables as a straight line. Instead, Logistic regression uses the natural logarithm function to find the relationship between the variables and uses test data to find the coefficients.  
Below is a sigmoid curve representing the Logistic model  

{{< figure library="true" src="logistic-regression-in-r/sigmoid_curve.png" alt="logistic regression sigmoid curve">}}  

Probability lies between **0** and **1**.  
In summary; we try to find the best fit **sigmoid** curve through the points of our data with Logistic Regression.  

In Logistic regression the `odds ratio` can be used to measure the strength between two events. An odds ratio of **1** implies that both events are independent, if the odds ratio is greater than **1** it means that the presence of event A raises the odds of event B, vice versa. if the odds ratio is less than **1** it means that the presence of event A reduces the odds of event B and vice versa.  

Now that we have an understanding of how the logistic regression works let us use an example to gain practical knowledge.  

## EXAMPLE  

For this example we will make use of the popular <a href="https://www.kaggle.com/c/titanic/data target=_blank rel=nofollow noopener"> titanic dataset </a> from kaggle, we will try to create a model with the Logistic regression to predict whether a passenger in the titanic boat survived or died (survived = 1, died = 0).  

Let's start by loading required packages 

```{r, message=FALSE}
require(tidyverse)
require(sjPlot)
#require(InformationValue)
```  

**Read in the train and test dataset**  

```{r, message=FALSE}
titanic_train <- read_csv("https://raw.githubusercontent.com/twirelex/dataset/master/train(1).csv")
titanic_test <- read_csv("https://raw.githubusercontent.com/twirelex/dataset/master/test(1).csv")
``` 

**Let's take a look at the datasets and see what we have**  

```{r}
glimpse(titanic_train)
```

```{r}
glimpse(titanic_test)
```
Notice that the train set has 11 independent variables with one dependent variable (`Survived`) while the test set has 11 independent variables without the dependent variable, this is because the test set is the unseen data that we want to predict on, so it does not have a dependent/response variable yet.  

**Let us view a summary statistics of these datasets to know if there are problems that we may need to first resolve before continuing**  

```{r}
summary(titanic_train)
```
```{r}
summary(titanic_test)
```


## DATA PREPARATION

It appears that there are missing values in both the train set and the test set and this can be a problem for our machine learning algorithm. So to fix this problem it will make sense if we first combine the train and test set into one dataframe and then go ahead to clean the combined dataframe as we see fit, and separate them again into train and test. We do not necessarily have to combine both dataset before performing the cleaning operation but in other for us not to repeat the same process on both dataset it will be more efficient if we first treat them as one.  

Combining the datasets is easy but we will need a way to be able to identify them separately when we want to split them back into train and test so that we don't end up having a mixed up train and test set that is different from our original sets.  

Let's create a variable called `identifier` where the train set will take values **TRUE** and the test set will take values **FALSE**, this way we will be able to differentiate the datasets after combining them.  
```{r}
titanic_train <- titanic_train %>% mutate(identifier = TRUE)

titanic_test <- titanic_test %>% mutate(identifier = FALSE)
```  

Now that we have created an `identifier` column, for us to be able to combine the datasets we need to make sure that we have the same number of variables in both datasets, remember that the `test` set doesn't have response/dependent variable. 
so we will create `Survived` variable with values that doesn't really matter and then combine the datasets.

```{r}
titanic_test <- titanic_test %>% mutate(Survived = 0)

dim(titanic_test)
```  

now we have the same number of variables in both datasets  

**Combine datasets**  

```{r}
titanic_combined <- titanic_train %>% bind_rows(titanic_test)

dim(titanic_combined)
```  

**To reduce computational time we are only going to use the independent variables  `Age`, `Pclass`, `SibSp`, `Sex`, `Parch`, `Fare`, `Embarked` in our model**  


```{r}
knitr::kable(head(titanic_combined))
```


Once again let us see the variables that has missing values  
```{r}
colSums(is.na(titanic_combined))
```

we can see that  `Age`, `Fare`, `Cabin` and `Embarked` has missing values

We could omit the observations with missing values and go on with modeling but there appears to be much missing values so omitting values could cause lost of vital patterns/information for our model.  

For numeric variables with missing values like `Age` and `Fare` we will replace all the missing values with the mean value,  for `Embarked` that has only 3 unique values we are going to replace the missing values with the most frequent among the 3 unique values, and for `Cabin` we won't bother replacing the missing values since we won't be making use of it in our model.  

**Replace missing values**  

```{r}
titanic_combined <- titanic_combined %>% 
  mutate(Age = replace_na(titanic_combined$Age, mean(titanic_combined$Age, na.rm = TRUE)))

titanic_combined <- titanic_combined %>% 
  mutate(Fare = replace_na(titanic_combined$Fare, mean(titanic_combined$Fare, na.rm = TRUE)))

titanic_combined <- titanic_combined %>% 
  mutate(Embarked = replace_na(titanic_combined$Embarked, "S")) # S has the highest frequency
```

Let us convert mis-represented variables to their right classes before splitting back the dataframe  

```{r}
titanic_combined <- titanic_combined %>% 
  mutate(Pclass = factor(Pclass), Sex = factor(Sex), Embarked = factor(Embarked))
```



Our next step will be to split the dataframe back into train and test  

```{r}
titanic_train <- titanic_combined %>% filter(identifier == TRUE)

titanic_test <- titanic_combined %>% filter(identifier == FALSE)
```  

**It is time to build our machine learning model with the Logistic Regression algorithm.**  


## BUILD MODEL  

In base r the `glm` function can be used to build a logistic regression model. glm stands for Generalized Linear Model  

**Build model with variables `Age`, `Pclass`, `SibSp`, `Sex`, `Parch`, `Fare`, `Embarked` on train set**  

```{r}
glm_model <- glm(factor(Survived) ~ Age + Pclass + SibSp + Sex + Parch + Fare + Embarked, data = titanic_train, family = "binomial")
```  
By specifying `family = "binomial"` we are simply indicating that we want to build a model where the dependent variable has a binary class.


**View summary of the model**  
```{r}
summary(glm_model)
```


View odds ratio for the different variables and their confidence interval
```{r}
cbind(OddsRatio = exp(coef(glm_model)), exp(confint(glm_model)))
```

We can also visualize the odds ratio using a forest plot of estimates
```{r, fig.height=3 ,hugoopts=list(alt="forest plot showing the odds ratio for the different variables.")}
plot_model(glm_model)
```  
Most of the variables falls at the left side of the plot, this means that they have a relationship with the response variable.

**Evaluate the performance of our model on the same train set with a ROC curve**  


```{r, fig.height=3 ,hugoopts=list(alt="ROC auc curve showing model performance")}
library(PRROC)
plot(roc.curve(predict(glm_model, titanic_train, type = "response"),titanic_train$Survived, curve = TRUE))
```



**Predict on test set**  

```{r}
test_predict <- predict(glm_model, titanic_test, type = "response")

head(test_predict) %>% knitr::kable()
```  



We get a probability score for our response variable when using glm, we can however set a threshold score depending on our preference. i.e we can classify all predictions with probability score greater than or equal to 0.5 as 1 and all probability score less than 0.5 as 0, where **1** indicates that a passenger survived and **0** means that the passenger didn't survive  

**Make a threshold**  
Now we will create a threshold of **0.5**. all values greater than or equal to **0.5** will be classified as **1** and **0** otherwise

```{r}
bind_cols(titanic_test[, c(3, 5, 6, 7, 8, 10, 12)], Survived = if_else(test_predict >= 0.5, 1, 0)) %>% head(10) %>% knitr::kable()
```  

With the simple example above we have successfully built a Logistic Regression model.















